# -*- coding: utf-8 -*-
"""Word Frequency Analyzer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11JF79w79mRZX2ZxMogtg65f0YXFE0zkR
"""

#!/usr/bin/env python3
"""
Word Frequency Analyzer
A command-line program that analyzes text files and reports word frequencies.

Usage:
    python word_frequency_analyzer.py <path_to_text_file> [options]

Options:
    --chart, -c     Generate a bar chart of the top words
    --top, -t N     Show top N words (default: 10)
    --help, -h      Show this help message

Example:
    python word_frequency_analyzer.py sample.txt --chart --top 15
"""

import argparse
import re
import sys
from collections import Counter
from pathlib import Path
import time


def clean_and_tokenize(text):
    """
    Clean text by removing punctuation and converting to lowercase.
    Split into individual words.

    Args:
        text (str): Raw text to clean and tokenize

    Returns:
        list: List of cleaned words
    """
    # Convert to lowercase and remove punctuation, keeping only alphanumeric and spaces
    cleaned = re.sub(r'[^\w\s]', ' ', text.lower())

    # Split into words and filter out empty strings
    words = [word for word in cleaned.split() if word.strip()]

    return words


def analyze_file(file_path, chunk_size=8192):
    """
    Efficiently analyze a text file for word frequencies.
    Processes large files in chunks to handle memory efficiently.

    Args:
        file_path (str): Path to the text file
        chunk_size (int): Size of chunks to read for large file processing

    Returns:
        Counter: Word frequency counter object

    Raises:
        FileNotFoundError: If the file doesn't exist
        PermissionError: If the file can't be read
    """
    try:
        file_path = Path(file_path)

        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")

        if not file_path.is_file():
            raise ValueError(f"Path is not a file: {file_path}")

        word_counter = Counter()

        # For efficient processing of large files
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
            buffer = ""

            while True:
                chunk = file.read(chunk_size)
                if not chunk:
                    # Process remaining buffer
                    if buffer:
                        words = clean_and_tokenize(buffer)
                        word_counter.update(words)
                    break

                # Add chunk to buffer
                buffer += chunk

                # Process complete words, keep incomplete word in buffer
                lines = buffer.split('\n')
                buffer = lines[-1]  # Keep last incomplete line

                # Process all complete lines
                for line in lines[:-1]:
                    words = clean_and_tokenize(line)
                    word_counter.update(words)

        return word_counter

    except FileNotFoundError:
        print(f"Error: File '{file_path}' not found.")
        sys.exit(1)
    except PermissionError:
        print(f"Error: Permission denied reading '{file_path}'.")
        sys.exit(1)
    except Exception as e:
        print(f"Error reading file: {e}")
        sys.exit(1)


def display_results(word_counter, top_n=10):
    """
    Display the top N most frequent words with their counts.

    Args:
        word_counter (Counter): Word frequency counter
        top_n (int): Number of top words to display
    """
    if not word_counter:
        print("No words found in the file.")
        return

    total_words = sum(word_counter.values())
    unique_words = len(word_counter)

    print(f"\nüìä ANALYSIS RESULTS")
    print(f"{'='*50}")
    print(f"Total words: {total_words:,}")
    print(f"Unique words: {unique_words:,}")
    print(f"\nTop {min(top_n, len(word_counter))} most frequent words:")
    print(f"{'-'*50}")

    # Get top words
    top_words = word_counter.most_common(top_n)

    # Calculate column widths for nice formatting
    max_word_length = max(len(word) for word, _ in top_words)
    max_count_length = len(str(top_words[0][1]))

    # Display results
    for i, (word, count) in enumerate(top_words, 1):
        percentage = (count / total_words) * 100
        print(f"{i:2d}. {word:<{max_word_length}} | "
              f"{count:>{max_count_length},} | {percentage:5.2f}%")


def generate_bar_chart(word_counter, top_n=10):
    """
    Generate a simple ASCII bar chart of the top words.

    Args:
        word_counter (Counter): Word frequency counter
        top_n (int): Number of top words to chart
    """
    try:
        import matplotlib.pyplot as plt
        import matplotlib.patches as mpatches

        top_words = word_counter.most_common(top_n)
        if not top_words:
            print("No data to chart.")
            return

        words, counts = zip(*top_words)

        # Create the plot
        plt.figure(figsize=(12, 8))
        bars = plt.bar(range(len(words)), counts,
                      color='steelblue', alpha=0.8, edgecolor='navy')

        plt.xlabel('Words', fontsize=12)
        plt.ylabel('Frequency', fontsize=12)
        plt.title(f'Top {len(words)} Most Frequent Words', fontsize=14, fontweight='bold')
        plt.xticks(range(len(words)), words, rotation=45, ha='right')

        # Add value labels on bars
        for bar, count in zip(bars, counts):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(counts)*0.01,
                    f'{count:,}', ha='center', va='bottom', fontsize=10)

        plt.grid(axis='y', alpha=0.3)
        plt.tight_layout()

        # Save chart
        chart_filename = 'word_frequency_chart.png'
        plt.savefig(chart_filename, dpi=300, bbox_inches='tight')
        print(f"\nüìà Bar chart saved as '{chart_filename}'")

        # Try to display if possible
        try:
            plt.show()
        except:
            print("Chart saved but cannot display in this environment.")

    except ImportError:
        # Fallback to ASCII chart
        print(f"\nüìà ASCII BAR CHART - Top {top_n} Words")
        print("="*60)

        top_words = word_counter.most_common(top_n)
        if not top_words:
            print("No data to chart.")
            return

        max_count = top_words[0][1]
        max_word_length = max(len(word) for word, _ in top_words)
        chart_width = 40

        for word, count in top_words:
            bar_length = int((count / max_count) * chart_width)
            bar = "‚ñà" * bar_length
            percentage = (count / sum(word_counter.values())) * 100

            print(f"{word:<{max_word_length}} | {bar:<{chart_width}} | "
                  f"{count:>6,} ({percentage:4.1f}%)")


def create_sample_file():
    """Create a sample text file for testing purposes."""
    sample_text = """
    The quick brown fox jumps over the lazy dog. The dog was sleeping peacefully
    under the warm sun. The brown fox was very clever and quick in its movements.
    Every morning, the fox would jump over the sleeping dog, and the dog never
    seemed to mind. This routine continued day after day, with the quick fox
    and the lazy dog becoming unlikely friends. The sun shone brightly on both
    the fox and the dog, creating a peaceful scene in the forest.

    In the forest, many animals lived together. The fox was known for being quick,
    while the dog was famous for being lazy but friendly. Other animals would often
    watch the daily routine of the fox jumping over the dog. The forest was full
    of life, with the sun providing warmth to all creatures.
    """

    with open('sample.txt', 'w') as f:
        f.write(sample_text)

    print("Sample file 'sample.txt' created for testing.")


def analyze_text_file(file_path, show_chart=False, top_n=10):
    """
    Convenient function for analyzing text files in notebook environments.

    Args:
        file_path (str): Path to the text file
        show_chart (bool): Whether to generate a bar chart
        top_n (int): Number of top words to display
    """
    print(f"üîç Analyzing file: {file_path}")
    start_time = time.time()

    # Analyze the file
    word_counter = analyze_file(file_path)

    # Display results
    display_results(word_counter, top_n)

    # Generate chart if requested
    if show_chart:
        generate_bar_chart(word_counter, top_n)

    # Show processing time
    end_time = time.time()
    print(f"\n‚è±Ô∏è  Analysis completed in {end_time - start_time:.2f} seconds")

    return word_counter


def main():
    """Main function to handle command-line arguments and coordinate the analysis."""
    # Check if running in Jupyter/Colab environment
    try:
        from IPython import get_ipython
        if get_ipython() is not None:
            # Running in Jupyter/Colab - provide helper functions instead
            print("üìù Word Frequency Analyzer loaded!")
            print("\nFor notebook usage, use these functions:")
            print("‚Ä¢ create_sample_file() - Create a sample text file")
            print("‚Ä¢ analyze_text_file('filename.txt') - Basic analysis")
            print("‚Ä¢ analyze_text_file('filename.txt', show_chart=True) - With chart")
            print("‚Ä¢ analyze_text_file('filename.txt', show_chart=True, top_n=20) - Custom top N")

            # Create sample file automatically for convenience
            create_sample_file()
            print("\n‚úÖ Sample file created! Try: analyze_text_file('sample.txt', show_chart=True)")
            return
    except ImportError:
        pass

    # Regular command-line usage
    parser = argparse.ArgumentParser(
        description="Analyze word frequencies in text files",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python word_frequency_analyzer.py document.txt
  python word_frequency_analyzer.py document.txt --chart
  python word_frequency_analyzer.py document.txt --top 20 --chart
  python word_frequency_analyzer.py --sample  # Create sample file for testing
        """
    )

    parser.add_argument('file_path', nargs='?', help='Path to the text file to analyze')
    parser.add_argument('--chart', '-c', action='store_true',
                       help='Generate a bar chart of word frequencies')
    parser.add_argument('--top', '-t', type=int, default=10, metavar='N',
                       help='Number of top words to display (default: 10)')
    parser.add_argument('--sample', action='store_true',
                       help='Create a sample text file for testing')

    args = parser.parse_args()

    # Handle sample file creation
    if args.sample:
        create_sample_file()
        return

    # Validate arguments
    if not args.file_path:
        parser.print_help()
        print("\nError: Please provide a file path to analyze.")
        sys.exit(1)

    if args.top < 1:
        print("Error: --top must be a positive integer.")
        sys.exit(1)

    # Use the analyze function
    analyze_text_file(args.file_path, args.chart, args.top)


if __name__ == "__main__":
    main()

with open('punct_test.txt', 'w') as f:
    f.write("Hello, world! How are you? I'm fine. Thanks!")

analyze_text_file('punct_test.txt')
# Should clean punctuation properly

"""
Word Frequency Analyzer with Bar Chart

Author: Nada Adel
"""

import re
from collections import Counter
from pathlib import Path
import matplotlib.pyplot as plt

WORD_PATTERN = re.compile(r"[^\w\s]")
WHITESPACE_PATTERN = re.compile(r"\s+")


def read_file(file_path: Path, encoding="utf-8") -> str:
    if not file_path.exists() or not file_path.is_file():
        raise FileNotFoundError(f"File not found: {file_path}")

    text_parts = []
    with file_path.open(encoding=encoding, errors="ignore") as f:
        for line in f:
            text_parts.append(line)
    return "".join(text_parts)


def extract_words(text: str, ignore_case=True, min_length=1) -> list[str]:
    cleaned = WORD_PATTERN.sub(" ", text)
    cleaned = WHITESPACE_PATTERN.sub(" ", cleaned)
    if ignore_case:
        cleaned = cleaned.lower()
    return [w for w in cleaned.split() if len(w) >= min_length]


def count_frequencies(words: list[str]) -> Counter:
    return Counter(words)


def display_top_words(counter: Counter, top_n=10) -> list[tuple[str, int]]:
    total = sum(counter.values())
    top_words = counter.most_common(top_n)
    for rank, (word, count) in enumerate(top_words, start=1):
        pct = (count / total) * 100
        print(f"{rank:>2}. {word:<15} {count:>6} ({pct:5.2f}%)")
    return top_words


def plot_bar_chart(top_words: list[tuple[str, int]]) -> None:
    words, counts = zip(*top_words)
    plt.figure(figsize=(8, 5))
    plt.bar(words, counts, color="skyblue")
    plt.xlabel("Words")
    plt.ylabel("Frequency")
    plt.title("Top Word Frequencies")
    plt.tight_layout()
    plt.show()


def main():
    file_input = input("Enter path to the text file: ").strip()
    file_path = Path(file_input)

    try:
        text = read_file(file_path)
        words = extract_words(text, min_length=1)
        counter = count_frequencies(words)

        print(f"\nFile: {file_path}")
        print(f"Total words: {sum(counter.values())}")
        print(f"Unique words: {len(counter)}\n")

        top_words = display_top_words(counter, top_n=10)

        plot_bar_chart(top_words)

    except FileNotFoundError as e:
        print(f"Error: {e}")


if __name__ == "__main__":
    main()